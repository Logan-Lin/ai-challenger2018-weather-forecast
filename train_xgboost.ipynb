{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "# from train_xgb import *\n",
    "from generate_set import *\n",
    "# from xgb_predict import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading netCDF4 file data/ai_challenger_wf2018_testb4_20180829-20181031.nc\n",
      "Loading station 90001\n",
      "Loading station 90002\n",
      "Loading station 90003\n",
      "Loading station 90004\n",
      "Loading station 90005\n",
      "Loading station 90006\n",
      "Loading station 90007\n",
      "Loading station 90008\n",
      "Loading station 90009\n",
      "Loading station 90010\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('train_data', '1015_1115_2.pkl'), 'rb') as pickle_file:\n",
    "    restore = pickle.load(pickle_file)\n",
    "X = restore['X']\n",
    "Y = restore['Y']\n",
    "dates = restore['dates']\n",
    "\n",
    "with open(os.path.join('validation_data', '1001_1030_2.pkl'), 'rb') as pickle_file:\n",
    "    restore_va = pickle.load(pickle_file)\n",
    "X_va = restore_va['X']\n",
    "Y_va = restore_va['Y']\n",
    "dates_va = restore_va['dates']\n",
    "\n",
    "t_raw = pd.read_pickle(os.path.join('dataframes', 'train_raw.pkl'))\n",
    "v_raw = pd.read_pickle(os.path.join('dataframes', 'validation_raw.pkl'))\n",
    "test_b1 = load_raw_file(os.path.join('data', 'ai_challenger_wf2018_testb4_20180829-20181031.nc'))\n",
    "a_raw = pd.concat([t_raw, v_raw, test_b1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Best: 0915_1015_1\n",
    "with open(os.path.join('parameters', '1015_1115_3.param'), 'rb') as file:\n",
    "    parameters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1th day models\n",
      "Training 2th day models\n",
      "Training 3th day models\n",
      "Training 4th day models\n",
      "Training 5th day models\n",
      "Training 6th day models\n",
      "Training 7th day models\n",
      "Training 8th day models\n",
      "Training 9th day models\n",
      "Training 10th day models\n",
      "Training 11th day models\n",
      "Training 12th day models\n",
      "Training 13th day models\n",
      "Training 14th day models\n",
      "Training 15th day models\n",
      "Training 16th day models\n",
      "Training 17th day models\n",
      "Training 18th day models\n",
      "Training 19th day models\n",
      "Training 20th day models\n",
      "Training 21th day models\n",
      "Training 22th day models\n",
      "Training 23th day models\n",
      "Training 24th day models\n",
      "Training 25th day models\n",
      "Training 26th day models\n",
      "Training 27th day models\n",
      "Training 28th day models\n",
      "Training 29th day models\n",
      "Training 30th day models\n",
      "Training 31th day models\n",
      "Training 32th day models\n",
      "Training 33th day models\n"
     ]
    }
   ],
   "source": [
    "# train_all_xgb(X, Y, X_va, Y_va, os.path.join('models', '1001_1101_2', '1013'), params=parameters)\n",
    "train_all_xgb(X, Y, X_va, Y_va, os.path.join('models', '1015_1115_2', '4'), params=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cycle_to_X(X, raw, column, predict_hour, add_days, interval=1):\n",
    "    explain = pd.read_csv(os.path.join('data', 'explain.csv'), index_col=0).dropna(how='any', axis=1)\n",
    "    \n",
    "    def get_station_id(X):\n",
    "        station_list = []\n",
    "        for index, row in X.iterrows():\n",
    "            station_onehot = row[['station_{}'.format(i) for i in range(10)]]\n",
    "            station_id = int('900%02d' % int(np.sum([station_onehot[i] * (i+1) for i in range(10)])))\n",
    "            station_list.append(station_id)\n",
    "        return station_list\n",
    "\n",
    "    def normalize_single_column(df: pd.DataFrame, name) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Normalize data.\n",
    "        \"\"\"\n",
    "        result = pd.DataFrame(df, copy=True)\n",
    "        scope = explain.loc[name]['scope'][1:-1].split(',')\n",
    "        min_value = float(scope[0].strip())\n",
    "        max_value = float(scope[1].strip())\n",
    "\n",
    "        result[name] = (result[name] - min_value) / (max_value - min_value)\n",
    "        return result\n",
    "    \n",
    "    origin_dates = [pd.Timestamp.fromtimestamp(timestamp) - pd.Timedelta('8 hours') for timestamp in X['timestamp']]\n",
    "    station_list = get_station_id(X)\n",
    "    series_list = []\n",
    "    for day in range(add_days):\n",
    "        day = (day + 1) * interval\n",
    "        fetch_dates = pd.Series(origin_dates) - pd.Timedelta('{} days'.format(day))\n",
    "        series = pd.Series([raw.loc[(station_id, date, predict_hour + 4)][column] \n",
    "                            for (station_id, date) in zip(station_list, fetch_dates)])\n",
    "        series.name = column\n",
    "        series_list.append(series)\n",
    "    result = normalize_single_column(pd.DataFrame(series_list).T, column)\n",
    "    result.columns = ['{}_{}*{}days'.format(column, interval, i + 1) for i in range(add_days)]\n",
    "    return pd.concat([X, result], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_xgb(X, Y, X_va, Y_va, dir_prefix, params, start=0, end=None):\n",
    "    \"\"\"\n",
    "    Train models for all time series and features.\n",
    "    \"\"\"\n",
    "    if end is None:\n",
    "        end = Y[0].shape[0]\n",
    "    y_train_list = {column: [] for column in Y[0].columns}\n",
    "    y_eval_list = {column: [] for column in Y[0].columns}\n",
    "    for i in range(start, end):\n",
    "        print('Training {}th day models'.format(i + 1))\n",
    "        for j in range(len(Y[0].columns)):\n",
    "            column = Y[0].columns[j]\n",
    "            \n",
    "            X_added = add_cycle_to_X(X, a_raw, column, i, 7)\n",
    "#             X_added = add_cycle_to_X(X_added, all_raw, column, i, 4, 7)\n",
    "            X_va_added = add_cycle_to_X(X_va, a_raw, column, i, 7)\n",
    "#             X_va_added = add_cycle_to_X(X_va_added, all_raw, column, i, 4, 7)\n",
    "            \n",
    "            y_train = fetch_labels(i, column, Y)\n",
    "            y_validation = fetch_labels(i, column, Y_va)\n",
    "            \n",
    "            max_depth = int(params.iloc[i*3+j]['max_depth']) \n",
    "            min_child_weight = int(params.iloc[i*3+j]['min_child_weight'])\n",
    "            xg_reg = xgb.XGBRegressor(max_depth=max_depth, learning_rate=0.15, n_estimators=150, silent=True, \n",
    "                                      objective='reg:linear', booster='gbtree', n_jobs=47, alpha=10,\n",
    "                                      gamma=0, min_child_weight=min_child_weight, max_delta_step=0, subsample=0.9, \n",
    "                                      colsample_bytree=0.9, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n",
    "                                      scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None)\n",
    "#             xg_reg.fit(add_previous_to_X(X, y_train_list[column]), y_train, \n",
    "#                        eval_set=[(add_previous_to_X(X_va, y_eval_list[column]), y_validation)], verbose=False, \n",
    "#                        eval_metric='rmse', early_stopping_rounds=15)\n",
    "            xg_reg.fit(X_added, y_train, \n",
    "                       eval_set=[(X_va_added, y_validation)], verbose=False, \n",
    "                       eval_metric='rmse', early_stopping_rounds=15)\n",
    "#             xg_reg.fit(X_added, y_train)\n",
    "#             xg_reg.fit(X, y_train)\n",
    "            \n",
    "            # Save model to file.\n",
    "            pickle.dump(xg_reg, open('{}_{}_{}.model'.format(dir_prefix, column, i), 'wb'))\n",
    "            \n",
    "#             y_train_list[column].append(y_train)\n",
    "#             y_eval_list[column].append(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
