{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "from generate_set import *\n",
    "from xgb_predict import *\n",
    "from train_xgb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading netCDF4 file data/ai_challenger_wf2018_testb7_20180829-20181103.nc\n",
      "Loading station 90001\n",
      "Loading station 90002\n",
      "Loading station 90003\n",
      "Loading station 90004\n",
      "Loading station 90005\n",
      "Loading station 90006\n",
      "Loading station 90007\n",
      "Loading station 90008\n",
      "Loading station 90009\n",
      "Loading station 90010\n"
     ]
    }
   ],
   "source": [
    "t_raw = pd.read_pickle(os.path.join('dataframes', 'train_raw.pkl'))\n",
    "v_raw = pd.read_pickle(os.path.join('dataframes', 'validation_raw.pkl'))\n",
    "test_b1 = load_raw_file(os.path.join('data', 'ai_challenger_wf2018_testb7_20180829-20181103.nc'))\n",
    "all_raw = pd.concat([t_raw, v_raw, test_b1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('dataframes', 'test_b6.pkl'), 'wb') as file:\n",
    "    pickle.dump(test_b1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this prediction date every day!\n",
    "predict_datetime = '2018-11-03 03:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loganlin/Downloads/ai-challenger/generate_set.py:69: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  history_list.append(df.loc[station_id, forecast_date - pd.DateOffset(days=i)].iloc[:24])\n",
      "/Users/loganlin/Downloads/ai-challenger/generate_set.py:70: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  history = pd.concat(history_list + [df.loc[station_id, forecast_date]])\n",
      "/Users/loganlin/Downloads/ai-challenger/generate_set.py:154: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  history_list.append(df.loc[station_id, forecast_date - pd.DateOffset(days=i)].iloc[:24])\n",
      "/Users/loganlin/Downloads/ai-challenger/generate_set.py:157: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  h = normalize(pd.concat(history_list + [df.loc[station_id, forecast_date]]))\n"
     ]
    }
   ],
   "source": [
    "x_oneday, y_oneday = generate_x_oneday(predict_datetime, all_raw)\n",
    "\n",
    "# With early stopping: 'models', '1015_1115_2', '2'\n",
    "# Without early stopping: 'models', '1015_1115_2', '3'\n",
    "# Modified validation set: 'models', '1015_1115_2', '4'\n",
    "pre_xgb = xgb_predict_all(x_oneday, y_oneday, os.path.join('models', '1015_1115_2', '4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_pre = load_numpy_prediction(os.path.join('numpy', 'pred_result_11_03.npy'))\n",
    "merge_pre = (dl_pre.sort_index() + pre_xgb.sort_index()) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loganlin/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:8: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sub = construct_submission(all_raw, dl_pre, predict_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_numpy_prediction(file_path):\n",
    "    \"\"\"\n",
    "    Turn Deep Learning output numpy array into the same format as XGBoost prediction result.\n",
    "    \"\"\"\n",
    "    pre_np = np.load(file_path)\n",
    "    pre_np = pre_np.reshape([3, 330]).T\n",
    "    indexes = pd.concat([pd.Series(['%d_%02d' % (station, seq) for seq in range(4, 37)]) for station in range(90001, 90011)])\n",
    "    indexes.name = 'FORE_data'\n",
    "    pre_df = pd.DataFrame(pre_np, columns=['t2m_obs', 'w10m_obs', 'rh2m_obs'], index=indexes)\n",
    "    pre_df = pre_df[['t2m_obs', 'rh2m_obs', 'w10m_obs']]\n",
    "    return pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_prediction(xgb_pre: pd.DataFrame, dl_pre: pd.DataFrame, merge_rule:pd.DataFrame, linear=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Using merge_rule to merge XGBoost prediction and Deep Learning prediction.\n",
    "    \"\"\"\n",
    "    xgb_pre = xgb_pre.sort_index()\n",
    "    dl_pre = dl_pre.sort_index()\n",
    "    merge_rule = merge_rule.sort_index()\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    station_list = list(range(90001, 90011))\n",
    "    for column in xgb_pre.columns:\n",
    "        result_column = []\n",
    "        for i in range(0, 10):\n",
    "            station = station_list[i]\n",
    "            rmse_diff = merge_rule.loc[station, column]\n",
    "            if rmse_diff > 0.5:\n",
    "                if linear:\n",
    "                    result_column.append((dl_pre.iloc[(i * 33):((i + 1) * 33)][column] + \n",
    "                                         xgb_pre.iloc[(i * 33):((i + 1) * 33)][column]) / 2)\n",
    "                else:\n",
    "                    result_column.append(dl_pre.iloc[(i * 33):((i + 1) * 33)][column])\n",
    "            else:\n",
    "                result_column.append(xgb_pre.iloc[(i * 33):((i + 1) * 33)][column])\n",
    "        result.append(pd.concat(result_column))\n",
    "    result = pd.DataFrame(result).T\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cycle_to_X(X, raw, column, predict_hour, add_days, interval=1):\n",
    "    explain = pd.read_csv(os.path.join('data', 'explain.csv'), index_col=0).dropna(how='any', axis=1)\n",
    "    \n",
    "    def get_station_id(X):\n",
    "        station_list = []\n",
    "        for index, row in X.iterrows():\n",
    "            station_onehot = row[['station_{}'.format(i) for i in range(10)]]\n",
    "            station_id = int('900%02d' % int(np.sum([station_onehot[i] * (i+1) for i in range(10)])))\n",
    "            station_list.append(station_id)\n",
    "        return station_list\n",
    "\n",
    "    def normalize_single_column(df: pd.DataFrame, name) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Normalize data.\n",
    "        \"\"\"\n",
    "        result = pd.DataFrame(df, copy=True)\n",
    "        scope = explain.loc[name]['scope'][1:-1].split(',')\n",
    "        min_value = float(scope[0].strip())\n",
    "        max_value = float(scope[1].strip())\n",
    "\n",
    "        result[name] = (result[name] - min_value) / (max_value - min_value)\n",
    "        return result\n",
    "    \n",
    "    origin_dates = [pd.Timestamp.fromtimestamp(timestamp) - pd.Timedelta('8 hours') for timestamp in X['timestamp']]\n",
    "    station_list = get_station_id(X)\n",
    "    series_list = []\n",
    "    for day in range(add_days):\n",
    "        day = (day + 1) * interval\n",
    "        fetch_dates = pd.Series(origin_dates) - pd.Timedelta('{} days'.format(day))\n",
    "        series = pd.Series([raw.loc[(station_id, date, predict_hour + 4)][column] \n",
    "                            for (station_id, date) in zip(station_list, fetch_dates)])\n",
    "        series.name = column\n",
    "        series_list.append(series)\n",
    "    result = normalize_single_column(pd.DataFrame(series_list).T, column)\n",
    "    result.columns = ['{}_{}*{}days'.format(column, interval, i + 1) for i in range(add_days)]\n",
    "    return pd.concat([X, result], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_predict_all(X, Y, dir_prefix):\n",
    "    \"\"\"\n",
    "    Predict all timestamps and features using xgb models.\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    for i in range(Y[0].shape[0]):\n",
    "        result_one_timestamp = []\n",
    "        for j in range(Y[0].shape[1]):\n",
    "            name = Y[0].columns[j]\n",
    "            \n",
    "            # X_added\n",
    "            X_added = add_cycle_to_X(X, all_raw, name, i, 7)\n",
    "#             X_added = add_cycle_to_X(X_added, all_raw, name, i, 4, 7)\n",
    "            \n",
    "            xg_reg = pickle.load(open('{}_{}_{}.model'.format(dir_prefix, name, i), 'rb'))\n",
    "            \n",
    "            # X_added\n",
    "            result = pd.Series(xg_reg.predict(X_added))\n",
    "    \n",
    "            indexes = ['%d_%02d' % (station_id, i + 4) for station_id in range(90001, 90011)]\n",
    "            result.index = indexes\n",
    "            result_one_timestamp.append(result)\n",
    "        result_list.append(pd.DataFrame(result_one_timestamp).T)\n",
    "    result = pd.concat(result_list)\n",
    "    result.columns = Y[0].columns\n",
    "    result.index.name = 'FORE_data'\n",
    "    result = denormalize(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_timespan(start, end, eval_set, dir_prefix, days=2):\n",
    "    \"\"\"\n",
    "    Use evaluation set to generate a summarize of \n",
    "    \"\"\"\n",
    "    start = pd.Timestamp(start)\n",
    "    end = pd.Timestamp(end)\n",
    "    y_list = []\n",
    "    pre_list = []\n",
    "    for date in pd.date_range(start, end):\n",
    "        x, y = generate_x_oneday(date, eval_set, days)\n",
    "        y_list.append(y)\n",
    "        pre = xgb_predict_all(x, y, dir_prefix)\n",
    "        pre_list.append(pre)\n",
    "    \n",
    "    # Calculate RMSE for each hour\n",
    "    hour_list = []\n",
    "    columns = None\n",
    "    for i in range(33):\n",
    "        pre_hour = pd.concat([pre.iloc[(i*10):((i+1)*10)] for pre in pre_list])\n",
    "        y_hour = denormalize(pd.concat([pd.DataFrame([y_one_station.iloc[i] for y_one_station in y]) for y in y_list]))\n",
    "        columns = y_hour.columns\n",
    "        row = []\n",
    "        for column in columns:\n",
    "            row.append(mse(pre_hour[column], y_hour[column]))\n",
    "        hour_list.append(row)\n",
    "    hour_rmse = pd.DataFrame(np.sqrt(hour_list), columns=columns)\n",
    "    \n",
    "    # Calculate RMSE for each station\n",
    "    station_list = []\n",
    "    for i in range(10):\n",
    "        pre_station = pd.concat([pre.sort_index().iloc[(i*33):((i+1)*33)] for pre in pre_list])\n",
    "        y_station = denormalize(pd.concat([y[i] for y in y_list]))\n",
    "        row = []\n",
    "        for column in columns:\n",
    "            row.append(mse(pre_station[column], y_station[column]))\n",
    "        station_list.append(row)\n",
    "    station_rmse = pd.DataFrame(np.sqrt(station_list), columns=columns, index=range(90001, 90011))\n",
    "    \n",
    "    # Calculate overall RMSE for three features\n",
    "    all_rmse = pd.DataFrame(np.sqrt(np.mean(station_list, axis=0)), index=columns)\n",
    "    \n",
    "    return hour_rmse, station_rmse, all_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/LinYan/ai-challenger/generate_set.py:69: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  history_list.append(df.loc[station_id, forecast_date - pd.DateOffset(days=i)].iloc[:24])\n",
      "/data/LinYan/ai-challenger/generate_set.py:70: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  history = pd.concat(history_list + [df.loc[station_id, forecast_date]])\n",
      "/data/LinYan/ai-challenger/generate_set.py:154: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  history_list.append(df.loc[station_id, forecast_date - pd.DateOffset(days=i)].iloc[:24])\n",
      "/data/LinYan/ai-challenger/generate_set.py:157: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  h = normalize(pd.concat(history_list + [df.loc[station_id, forecast_date]]))\n"
     ]
    }
   ],
   "source": [
    "hour_rmse, station_rmse, all_rmse = eval_timespan('2018-10-25 03:00:00', '2018-10-26 03:00:00', \n",
    "                                                  all_raw, os.path.join('models', '1015_1115_2', '2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl_rmse = [[ 1.8833348,3.49686628,3.8517941 ,  3.47419742 , 1.25979552 , 4.76592387,\n",
    "#    3.83747535 , 1.94121611 , 3.57586791  ,3.21245561],\n",
    "#  [ 1.39719772,  1.24978646  ,1.2362636  , 1.33176675 , 1.45978062 , 1.27638727,\n",
    "#    1.87945335 , 1.68263699 , 1.08730759 , 1.64784673],\n",
    "#  [15.07691953, 11.74088683, 16.04245051, 14.7884658 , 13.68954934, 23.22244553,\n",
    "#    8.15870517 , 4.72383667,  5.80455138 ,15.42624515]]\n",
    "# dl_rmse = np.array(dl_rmse).T\n",
    "dl_rmse = np.load(os.path.join('numpy', 'final_rmse.npy')).T\n",
    "dl_rmse = pd.DataFrame(dl_rmse, index=range(90001, 90011), columns=['t2m_obs' ,'w10m_obs' ,'rh2m_obs'])\n",
    "dl_rmse = dl_rmse[['t2m_obs', 'rh2m_obs', 'w10m_obs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(y, prediction):\n",
    "    rmse_matrix = []\n",
    "    for i in range(len(y)):\n",
    "        rmse_one = []\n",
    "        plt.figure(figsize=(20, 3))\n",
    "        for j in range(3):\n",
    "            column = prediction.columns[j]\n",
    "            plt.subplot(1, 3, j+1, ylabel=column)\n",
    "            if j == 1:\n",
    "                plt.title(range(90001, 90011)[i])\n",
    "            pre_one = prediction.sort_index().iloc[33*i:33*(i+1)][column]\n",
    "            y_one = denormalize(y[i])[column]\n",
    "            plt.plot(range(33), pre_one, color='red', label='pre')\n",
    "            plt.plot(range(33), y_one, color='green', label='true')\n",
    "            rmse_one.append(sqrt(mse(pre_one, y_one)))\n",
    "            plt.legend()\n",
    "        rmse_matrix.append(rmse_one)\n",
    "    return pd.DataFrame(rmse_matrix, columns=prediction.columns, index=range(90001, 90011))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_pre = load_numpy_prediction(os.path.join('numpy', 'pred_result_10_28.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/LinYan/ai-challenger/generate_set.py:69: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  history_list.append(df.loc[station_id, forecast_date - pd.DateOffset(days=i)].iloc[:24])\n",
      "/data/LinYan/ai-challenger/generate_set.py:70: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  history = pd.concat(history_list + [df.loc[station_id, forecast_date]])\n",
      "/data/LinYan/ai-challenger/generate_set.py:154: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  history_list.append(df.loc[station_id, forecast_date - pd.DateOffset(days=i)].iloc[:24])\n",
      "/data/LinYan/ai-challenger/generate_set.py:157: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  h = normalize(pd.concat(history_list + [df.loc[station_id, forecast_date]]))\n"
     ]
    }
   ],
   "source": [
    "x_1015, y_1015 = generate_x_oneday('2018-10-15 03:00:00', all_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_submission(raw_data, prediction, forecast_date, file_dir=None):\n",
    "    \"\"\"\n",
    "    Construct submission data frame using original data and forecast result.\n",
    "    \"\"\"\n",
    "    forecast_date = pd.Timestamp(forecast_date)\n",
    "    origin_list = []\n",
    "    for station_id in range(90001, 90011):\n",
    "        origin_data = raw_data.loc[station_id, forecast_date][prediction.columns].iloc[:4]\n",
    "        origin_data = origin_data.interpolate(limit_direction='both')\n",
    "        origin_data.index = ['%d_%02d' % (station_id, i) for i in range(4)]\n",
    "        origin_list.append(origin_data)\n",
    "    origin = pd.concat(origin_list)\n",
    "    submission = pd.concat([origin, prediction])\n",
    "    submission.columns = [name.split('_')[0] for name in submission.columns]\n",
    "    submission.index.name = 'FORE_data'\n",
    "    submission = submission.sort_index()\n",
    "\n",
    "    if file_dir is None:\n",
    "        file_dir = os.path.join('submission', 'forecast-{}.csv'.format(forecast_date.strftime('%Y%m%d%H')))\n",
    "    with open(file_dir, 'w') as f:\n",
    "        f.write('%10s,%10s,%10s,%10s' % ('FORE_data', 't2m', 'rh2m', 'w10m'))\n",
    "        for index, value in submission.iterrows():\n",
    "            f.write('\\n%10s,%10.5f,%10.5f,%10.5f' % (index, value['t2m'], value['rh2m'], value['w10m']))\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
